<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Jack Baker" />


<title>sgmcmc: Getting Started</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">sgmcmc: Getting Started</h1>
<h4 class="author"><em>Jack Baker</em></h4>



<p>The goal of <code>sgmcmc</code> is to make it as easy as possible for users to run stochastic gradient MCMC (SGMCMC) algorithms. SGMCMC are algorithms which enable MCMC to scale more easily to large datasets, as traditional MCMC can run very slowly as dataset sizes grow.</p>
<p><code>sgmcmc</code> implements a lot of the popular stochastic gradient MCMC methods including <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Welling_398.pdf">SGLD</a>, <a href="https://arxiv.org/pdf/1402.4102v2.pdf">SGHMC</a> and <a href="http://papers.nips.cc/paper/5592-bayesian-sampling-using-stochastic-gradient-thermostats.pdf">SGNHT</a>. The package uses automatic differentiation, so all the differentiation needed for the methods is calculated automatically. Control variate methods can be used in order to improve the efficiency of the methods as proposed in the <a href="https://github.com/jbaker92/stochasticGradientMCMC">recent publication</a>. This package is designed to be user friendly. In order to execute these algorithms, users only need to specify the data; the log-likelihood function and log-prior density; the parameter starting values; and a few tuning parameters.</p>
<p>To enable as much flexibility as possible, the data and parameter starting points fed to the functions in <code>sgmcmc</code> are specified as lists. This allows users to specify multiple parameters and datasets. It also allows the user to easily reference these quantities in the log-likelihood function and log-prior density, and to set different stepsizes for different paramters (essential when parameters are on different scales).</p>
<div id="specifying-the-data" class="section level3">
<h3>Specifying the Data</h3>
<p>As we mentioned earlier, the datasets you wish to use are specified as a list. Suppose we have datasets we have already obtained or created <code>X</code> and <code>Y</code>, we would specify the whole dataset for our session as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dataset =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</code></pre></div>
<p>You can specify as many datasets as you like, the most important thing is that your naming is consistent, so you use the same names in your log-likelihood and log-prior functions, and in your list of stepsizes.</p>
<p>The functions assume that each observation is located on the first axis of the object. Suppose <code>Y</code> is a 2d matrix, then the observation <span class="math inline">\(Y_i\)</span> should be located at <code>dataset$Y[i,]</code>. Similarly if <code>X</code> is a 3d array, observation <span class="math inline">\(X_i\)</span> should be located at <code>dataset$X[i,,]</code>.</p>
</div>
<div id="specifying-the-parameters" class="section level3">
<h3>Specifying the Parameters</h3>
<p>Again parameters are specified as a list, the names are what we will refer to them as in the <code>logLik</code> and <code>logPrior</code> functions introduced below, the values are the desired starting points. Suppose my model depends on two parameter vectors <code>theta1</code> and <code>theta2</code>, and we want to start both from 0. If we assume both are length 3, this could be specified like this</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;theta1&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="st">&quot;theta2&quot;</span> =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>))</code></pre></div>
</div>
<div id="specifying-the-log-likelihood-and-log-prior" class="section level3">
<h3>Specifying the Log Likelihood and Log Prior</h3>
<p>The log-likelihood function is specified as a function of the <code>dataset</code> and <code>params</code>, which have the same names as the lists you just specified. The only difference is that the objects inside the lists will have automatically been converted to TensorFlow objects for you. The <code>dataset</code> list will contain TensorFlow placeholders. The <code>params</code> list will contain TensorFlow variables. The <code>logLik</code> function should be a function that takes these lists as input and returns what the log-likelihood value should be given the current parameter and data values. It should do this using TensorFlow operations. More details about how TensorFlow works in <code>R</code> can be found <a href="https://tensorflow.rstudio.com/">here</a>.</p>
<p>The log-prior density is specified in exactly the same way except that the function should only take <code>params</code> as input, as a prior should be independent of the dataset. You do not have to specify the prior at all, and this leads to the algorithm using a uniform, uninformative prior.</p>
<p>Suppose we want to simulate from the mean of a multivariate Normal density with each component of the mean having a Student-T prior, we would specify this as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
<span class="co"># Simulate and declare dataset</span>
dataset =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">10</span>^<span class="dv">4</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="kw">diag</span>(<span class="dv">2</span>)))
<span class="co"># Simulate random starting point</span>
params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;theta&quot;</span> =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">2</span>))

<span class="co"># Declare log likelihood</span>
logLik =<span class="st"> </span>function(params, dataset) {
    <span class="co"># Declare distribution, assuming Sigma known and constant</span>
    SigmaDiag =<span class="st"> </span><span class="kw">c</span>( <span class="dv">1</span>, <span class="dv">1</span> )
    distn =<span class="st"> </span>tf$contrib$distributions$<span class="kw">MultivariateNormalDiag</span>(params$theta, SigmaDiag)
    <span class="co"># Return sum of log pdf</span>
    <span class="kw">return</span>(tf$<span class="kw">reduce_sum</span>(distn$<span class="kw">log_prob</span>(dataset$X)))
}

<span class="co"># Declare log prior</span>
logPrior =<span class="st"> </span>function(params) {
    <span class="co"># Declare prior distribution</span>
    distn =<span class="st"> </span>tf$contrib$distributions$<span class="kw">StudentT</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>)
    <span class="co"># Apply log prior componentwise and return sum</span>
    <span class="kw">return</span>(tf$<span class="kw">reduce_sum</span>(distn$<span class="kw">log_prob</span>(params$theta)))
}</code></pre></div>
<p>Most of the time just specifying the constants in these functions, such as <code>SigmaDiag</code> as <code>R</code> objects will be fine. But occassionally there are issues when these constants get automatically converted to <code>tf$float64</code> objects by <code>TensorFlow</code> rather than <code>tf$float32</code>. If you run into errors involving <code>tf$float64</code> then force the constants to be input as <code>tf$float32</code> by using <code>SigmaDiag = tf$constant( c( 1, 1 ), dtype = tf$float32 )</code>.</p>
</div>
<div id="specifying-the-tuning-parameters" class="section level3">
<h3>Specifying the Tuning Parameters</h3>
<p>The only other input that needs to be specified to set any of the standard stochastic gradient MCMC methods running is the <code>stepsize</code>. This is normally defined as a list with an entry for each parameter name as follows</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stepsize =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;theta = 1e-5&quot;</span> )</code></pre></div>
<p>A short hand is just to set <code>stepsize = 1e-5</code> which just sets the stepsize of each parameter to be <code>1e-5</code>. This shorthand can be used for any of the tuning constants.</p>
<p>So to get <code>sgld</code> working for the multivariate Normal log-likelihood function we specified, and an uninformative uniform prior, we can simply run</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sgld</span>( logLik, dataset, params, stepsize )</code></pre></div>
<p>similarly for <code>sghmc</code> and <code>sgnht</code>.</p>
<p>To use the Student-t prior we specified, and set a minibatch size of <code>500</code>, we’d use</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sgld</span>( logLik, dataset, params, stepsize, <span class="dt">logPrior =</span> logPrior, <span class="dt">minibatchSize =</span> <span class="dv">500</span> )</code></pre></div>
<p>For more details of the other optional arguments see the main documentation in the <a href="https://stor-i.github.io/sgmcmc/reference/index.html">reference</a>.</p>
<p>All of the <a href="https://arxiv.org/pdf/1706.05439.pdf">control variate methods</a> <code>sgldcv</code>, <code>sghmccv</code> and <code>sgnhtcv</code> require an extra input <code>optStepsize</code>, which is the stepsize tuning constant for the initial optimization to find the MAP estimates. This argument is simply a small numeric value, such as <code>0.1</code>, and may require some tuning, which we talk about below. To run <code>sgldcv</code> on the multivariate Normal model, with specified prior we’d use</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">optStepsize =<span class="st"> </span><span class="fl">0.1</span>
<span class="kw">sgldcv</span>( logLik, dataset, params, stepsize, optStepsize, <span class="dt">logPrior =</span> logPrior, <span class="dt">minibatchSize =</span> <span class="dv">500</span> )</code></pre></div>
<p>similar for <code>sghmccv</code> and <code>sgnhtcv</code>.</p>
<p>Most of the time, parameters need tuning, we suggest doing this using cross validation. You can roughly check the algorithm is converging by inspection by checking that the <code>Log-posterior estimate</code> output by the algorithm settles down eventually (it should decrease at first unless the chain converges very quickly).</p>
</div>
<div id="next-steps" class="section level3">
<h3>Next Steps</h3>
<p>We suggest for more details you read the worked examples in the Articles section, these cover a variety of models (which will be expanded as the package matures): - <a href="https://stor-i.github.io/sgmcmc///articles/mvGauss.html">Multivariate Gaussian</a> - <a href="https://stor-i.github.io/sgmcmc///articles/gaussMixture.html">Gaussian Mixture</a> - <a href="https://stor-i.github.io/sgmcmc///articles/logisticRegression.html">Logistic Regression</a></p>
<p>The SGMCMC algorithms can also be run step by step, which allows custom storage of parameters using test functions, or sequential estimates. Useful if your chain is too large to fit into memory! This requires a better knowledge of TensorFlow. An example of this is given in the <a href="https://stor-i.github.io/sgmcmc///articles/nn.html">neural network</a> vignette.</p>
<p>Full details of the API can be found <a href="https://stor-i.github.io/sgmcmc///reference/index.html">here</a>.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
